{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: monthdelta in /Users/haolu/anaconda3/envs/tensorflow/lib/python3.6/site-packages (0.9.1)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install monthdelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Package\n",
    "import bs4 as bs\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "import monthdelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chrome Driver\n",
    "# executable_path = {'executable_path':'/usr/local/bin/chromedriver'}\n",
    "# browser = Browser('chrome',**executable_path,headless=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Yahoo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping Function\n",
    "* Input: \n",
    "    * a) Ticker Name, \n",
    "    * b) Start date, in datetime format; \n",
    "    * c) End date, in datetime format; \n",
    "    * d) Optional, data frequency:\n",
    "        * d.1) 1d (every business day); \n",
    "        * d.2) 1wk (every week); \n",
    "        * d.3) 1mo (every month)\n",
    "* Output: Dataframe with the following column names:\n",
    "    * 1) Open Price\n",
    "    * 2) High Price\n",
    "    * 3) Low Price\n",
    "    * 4) Close Price\n",
    "    * 5) Adj Close Price\n",
    "    * 6) Volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_historical_price(ticker, date1, date2, frequency='1d',display=True):\n",
    " \n",
    "    format_string='%Y-%m-%d %H:%M:%S'\n",
    " \n",
    "    # One day (86400 second) adjustment required to get dates printed to match web site manual output\n",
    "    _date1 = date1.strftime(\"%Y-%m-%d 00:00:00\")\n",
    "    date1_epoch = str(int(time.mktime(time.strptime(_date1, format_string)))- 86400)\n",
    "    \n",
    "    if display == True: \n",
    "        print(\"\")\n",
    "        print(date1, date1_epoch, \" + 86,400 = \", str(int(date1_epoch) + 86400))\n",
    " \n",
    "    _date2 = date2.strftime(\"%Y-%m-%d 00:00:00\")\n",
    "    date2_epoch = str(int(time.mktime(time.strptime(_date2, format_string))))\n",
    "    \n",
    "    if display == True:\n",
    "        print(date2, date2_epoch)\n",
    " \n",
    "    url = 'https://finance.yahoo.com/quote/' + ticker + '/history?period1=' + date1_epoch + '&period2=' + date2_epoch + '&interval='+frequency+'&filter=history&frequency='+frequency\n",
    "    source = urllib.request.urlopen(url).read()      \n",
    "    soup = bs.BeautifulSoup(source,'lxml')\n",
    "    table_rows = soup.find_all('tr')\n",
    "      \n",
    "    extract_table = []\n",
    "      \n",
    "    for table_row in table_rows:\n",
    "        table_row_values = table_row.find_all('td')\n",
    "        extract_row = [i.text for i in table_row_values]\n",
    "        extract_table.append(extract_row)        \n",
    "      \n",
    "    column_names = ['Date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']\n",
    "  \n",
    "    # data = data[1:-2]\n",
    "    extract_table_df = pd.DataFrame(extract_table)\n",
    "    extract_table_df.columns = column_names\n",
    "    extract_table_df.set_index(column_names[0], inplace=True)\n",
    "    extract_table_df = extract_table_df.convert_objects(convert_numeric=True)\n",
    "    extract_table_df = extract_table_df.iloc[::-1]\n",
    "    extract_table_df.dropna(inplace=True)\n",
    "      \n",
    "    return extract_table_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping Function Trial with Apple Stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Today     : 2019-02-19\n",
      "Start Date: 2018-06-02 Start Date Epoch: 1527915600\n",
      "End   Date: 2019-02-19 End   Date Epoch: 1550556000\n",
      "Processing 2018-06-02 thru 2018-07-31.\n",
      "\n",
      "2018-06-02 1527829200  + 86,400 =  1527915600\n",
      "2018-07-31 1533013200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haolu/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:33: FutureWarning: convert_objects is deprecated.  To re-infer data dtypes for object columns, use DataFrame.infer_objects()\n",
      "For all other conversions use the data-type specific converters pd.to_datetime, pd.to_timedelta and pd.to_numeric.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2018-08-01 thru 2018-09-30.\n",
      "\n",
      "2018-08-01 1533013200  + 86,400 =  1533099600\n",
      "2018-09-30 1538283600\n",
      "Processing 2018-10-01 thru 2018-11-30.\n",
      "\n",
      "2018-10-01 1538283600  + 86,400 =  1538370000\n",
      "2018-11-30 1543557600\n",
      "Processing 2018-12-01 thru 2019-01-31.\n",
      "\n",
      "2018-12-01 1543557600  + 86,400 =  1543644000\n",
      "2019-01-31 1548914400\n",
      "Processing 2019-02-01 thru 2019-02-19.\n",
      "\n",
      "2019-02-01 1548914400  + 86,400 =  1549000800\n",
      "2019-02-19 1550556000\n",
      "                Open    High     Low   Close  Adj Close      Volume\n",
      "Date                                                               \n",
      "Jun 01, 2018  187.99  190.26  187.75  190.24     188.11  23,442,500\n",
      "Jun 04, 2018  191.64  193.42  191.35  191.83     189.68  26,266,200\n",
      "Jun 05, 2018  193.07  193.94  192.36  193.31     191.14  21,566,000\n",
      "Jun 06, 2018  193.63  194.08  191.92  193.98     191.81  20,933,600\n",
      "Jun 07, 2018  194.14  194.20  192.34  193.46     191.29  21,347,200\n",
      "Jun 08, 2018  191.17  192.00  189.77  191.70     189.55  26,656,800\n",
      "Jun 11, 2018  191.35  191.97  190.21  191.23     189.09  18,308,500\n",
      "Jun 12, 2018  191.39  192.61  191.15  192.28     190.13  16,911,100\n",
      "Jun 13, 2018  192.42  192.88  190.44  190.70     188.56  21,638,400\n",
      "Jun 14, 2018  191.55  191.57  190.22  190.80     188.66  21,610,100\n",
      "Jun 15, 2018  190.03  190.16  188.26  188.84     186.72  61,719,200\n",
      "Jun 18, 2018  187.88  189.22  187.20  188.74     186.63  18,484,900\n",
      "Jun 19, 2018  185.14  186.33  183.45  185.69     183.61  33,578,500\n",
      "Jun 20, 2018  186.35  187.20  185.73  186.50     184.41  20,628,700\n",
      "Jun 21, 2018  187.25  188.35  184.94  185.46     183.38  25,711,900\n",
      "Jun 22, 2018  186.12  186.15  184.70  184.92     182.85  27,200,400\n",
      "Jun 25, 2018  183.40  184.92  180.73  182.17     180.13  31,663,100\n",
      "Jun 26, 2018  182.99  186.53  182.54  184.43     182.36  24,569,200\n",
      "Jun 27, 2018  185.23  187.28  184.03  184.16     182.10  25,285,300\n",
      "Jun 28, 2018  184.10  186.21  183.80  185.50     183.42  17,365,200\n",
      "Jun 29, 2018  186.29  187.19  182.91  185.11     183.04  22,737,700\n",
      "Jul 02, 2018  183.82  187.30  183.42  187.18     185.08  17,731,300\n",
      "Jul 03, 2018  187.79  187.95  183.54  183.92     181.86  13,954,800\n",
      "Jul 05, 2018  185.26  186.41  184.28  185.40     183.32  16,604,200\n",
      "Jul 06, 2018  185.42  188.43  185.20  187.97     185.86  17,485,200\n",
      "Jul 09, 2018  189.50  190.68  189.30  190.58     188.45  19,756,600\n",
      "Jul 10, 2018  190.71  191.28  190.18  190.35     188.22  15,939,100\n",
      "Jul 11, 2018  188.50  189.78  187.61  187.88     185.78  18,831,500\n",
      "Jul 12, 2018  189.53  191.41  189.31  191.03     188.89  18,041,100\n",
      "Jul 13, 2018  191.08  191.84  190.90  191.33     189.19  12,513,900\n",
      "...              ...     ...     ...     ...        ...         ...\n",
      "Jan 08, 2019  149.56  151.82  148.52  150.75     150.11  41,025,300\n",
      "Jan 09, 2019  151.29  154.53  149.63  153.31     152.66  45,099,100\n",
      "Jan 10, 2019  152.50  153.97  150.86  153.80     153.14  35,780,700\n",
      "Jan 11, 2019  152.88  153.70  151.51  152.29     151.64  27,023,200\n",
      "Jan 14, 2019  150.85  151.27  149.22  150.00     149.36  32,439,200\n",
      "Jan 15, 2019  150.27  153.39  150.05  153.07     152.42  28,710,900\n",
      "Jan 16, 2019  153.08  155.88  153.00  154.94     154.28  30,569,700\n",
      "Jan 17, 2019  154.20  157.66  153.26  155.86     155.19  29,821,200\n",
      "Jan 18, 2019  157.50  157.88  155.98  156.82     156.15  33,751,000\n",
      "Jan 22, 2019  156.41  156.73  152.62  153.30     152.65  30,394,000\n",
      "Jan 23, 2019  154.15  155.14  151.70  153.92     153.26  23,130,600\n",
      "Jan 24, 2019  154.11  154.48  151.74  152.70     152.05  25,441,500\n",
      "Jan 25, 2019  155.48  158.13  154.32  157.76     157.09  33,535,500\n",
      "Jan 28, 2019  155.79  156.33  153.66  156.30     155.63  26,192,100\n",
      "Jan 29, 2019  156.25  158.13  154.11  154.68     154.02  41,587,200\n",
      "Jan 30, 2019  163.25  166.15  160.23  165.25     164.54  61,109,800\n",
      "Jan 31, 2019  166.11  169.00  164.56  166.44     165.73  40,739,600\n",
      "Jan 31, 2019  166.11  169.00  164.56  166.44     165.73  40,739,600\n",
      "Feb 01, 2019  166.96  168.98  165.93  166.52     165.81  32,668,100\n",
      "Feb 04, 2019  167.41  171.66  167.28  171.25     170.52  31,495,500\n",
      "Feb 05, 2019  172.86  175.08  172.35  174.18     173.44  36,101,600\n",
      "Feb 06, 2019  174.65  175.57  172.85  174.24     173.50  28,239,600\n",
      "Feb 07, 2019  172.40  173.94  170.34  170.94     170.21  31,741,700\n",
      "Feb 08, 2019  168.99  170.66  168.42  170.41     170.41  23,820,000\n",
      "Feb 11, 2019  171.05  171.21  169.25  169.43     169.43  20,993,400\n",
      "Feb 12, 2019  170.10  171.00  169.70  170.89     170.89  22,283,500\n",
      "Feb 13, 2019  171.39  172.48  169.92  170.18     170.18  22,490,200\n",
      "Feb 14, 2019  169.71  171.26  169.38  170.80     170.80  21,835,700\n",
      "Feb 15, 2019  171.25  171.70  169.75  170.42     170.42  24,626,800\n",
      "Feb 19, 2019  169.71  171.44  169.49  170.93     170.93  18,962,100\n",
      "\n",
      "[183 rows x 6 columns]\n",
      "len of whole extracted data set = 183\n"
     ]
    }
   ],
   "source": [
    "## Try Scraping with APPL. \n",
    "\n",
    "# Initialize the end date to be today and start date is one year before. \n",
    "print(\"\")\n",
    "print(\"\")\n",
    "start_date = datetime.date(2018, 6, 2)\n",
    "end_date = datetime.date(2019, 2, 19)\n",
    "today = datetime.date.today()\n",
    " \n",
    "# The statements in this group are for debugging purposes only\n",
    "format_string='%Y-%m-%d %H:%M:%S'\n",
    "t1 = start_date.strftime(\"%Y-%m-%d 00:00:00\")\n",
    "t2 = end_date.strftime(\"%Y-%m-%d 00:00:00\")\n",
    "start_date_epoch = str(int(time.mktime(time.strptime(t1, format_string))))\n",
    "end_date_epoch = str(int(time.mktime(time.strptime(t2,format_string))))\n",
    " \n",
    "# Output all 'original' dates\n",
    "print('Today     :', today)\n",
    "print('Start Date:', start_date, 'Start Date Epoch:', start_date_epoch)\n",
    "print('End   Date:', end_date,   'End   Date Epoch:', end_date_epoch)\n",
    " \n",
    "# Initialize 'date1'\n",
    "date1 = start_date\n",
    " \n",
    "# Initialize 'date1'\n",
    "date1 = start_date\n",
    " \n",
    "# Do not allow the 'End Date' to be AFTER today\n",
    "if today < end_date:\n",
    "    end_date = today\n",
    "\n",
    "iteration_number = 0\n",
    "while date1 <= end_date:\n",
    "    iteration_number += 1\n",
    " \n",
    "    # Create 'date2' in a 60 day Window or less\n",
    "    date2 = date1 + monthdelta.monthdelta(2)\n",
    "    date2 = datetime.date(date2.year, date2.month, 1)\n",
    "    date2 = date2 - datetime.timedelta(days=1)\n",
    "         \n",
    "    # Do not allow 'date2' to go beyond the 'End Date'\n",
    "    if date2 > end_date:\n",
    "        date2 = end_date\n",
    "         \n",
    "    print(f\"Processing {date1} thru {date2}.\")\n",
    "    stock_symbol = 'AAPL'\n",
    "    df = get_historical_price(stock_symbol, date1, date2)\n",
    "     \n",
    "    if iteration_number == 1:\n",
    "        dfall = df.copy()\n",
    "    else:\n",
    "        frames = [dfall, df]\n",
    "        dfall = pd.concat(frames)\n",
    " \n",
    "    # # # print(dfall)\n",
    "    # # # print(\"len of dfall = {}\".format(len(dfall)))\n",
    " \n",
    "    # Increment the first date for the next pass\n",
    "    date1 = date1   + monthdelta.monthdelta(2)\n",
    "    date1 = datetime.date(date1.year, date1.month, 1)\n",
    "\n",
    "# Output concatenated data set\n",
    "print(dfall)\n",
    "print(f\"len of whole extracted data set = {len(dfall)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping with Short and Long Term Bond ETFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ETF Tickers\n",
    "* Long Term Bond ETFs:\n",
    "    * IEF --> iShares Barclays 7-10 Year Trasry Bnd Fd\n",
    "    * DTYL --> BARCLAY BK IPAT US TR 10 YR BULL ETN\n",
    "    * EDV --> VANGUARD WORLD/EXTD DURATION TREAS\n",
    "    * TLH --> iShares 10-20 Year Treasury Bond ETF\n",
    "* Long Term Bond ETFs:\n",
    "    * SHV --> iShares Short Treasury Bond ETF \n",
    "    * VGSH --> Vanguard Short-Term Treasury ETF\n",
    "    * SCHR --> Schwab Intermediate-Term US Trs ETF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "Short_TR_Tickers = ['IEF','DTYL','EDV','TLH']\n",
    "Long_TR_Tickers = ['SHV','VGSH','SCHR']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop through each ticker to scrape data, Scrape from 2000 to today.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Scraping historical data from  2000-01-02  to  2019-02-19\n",
      "Scraping ticker is IEF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haolu/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:37: FutureWarning: convert_objects is deprecated.  To re-infer data dtypes for object columns, use DataFrame.infer_objects()\n",
      "For all other conversions use the data-type specific converters pd.to_datetime, pd.to_timedelta and pd.to_numeric.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data extracted = 96\n",
      "\n",
      "\n",
      "Scraping historical data from  2000-01-02  to  2019-02-19\n",
      "Scraping ticker is DTYL\n",
      "Number of data extracted = 9\n",
      "\n",
      "\n",
      "Scraping historical data from  2000-01-02  to  2019-02-19\n",
      "Scraping ticker is EDV\n",
      "Number of data extracted = 99\n",
      "\n",
      "\n",
      "Scraping historical data from  2000-01-02  to  2019-02-19\n",
      "Scraping ticker is TLH\n",
      "Number of data extracted = 96\n"
     ]
    }
   ],
   "source": [
    "# Scrape Short Term Bonds\n",
    "Short_TR_Data_Dict = {}\n",
    "\n",
    "for ticker in Short_TR_Tickers:\n",
    "    \n",
    "    # Set up start and end date\n",
    "    start_date = datetime.date(2000, 1, 2)\n",
    "    end_date = datetime.date.today()\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"\")\n",
    "    print ('Scraping historical data from ',start_date,' to ',end_date)\n",
    "    print (f'Scraping ticker is {ticker}')\n",
    "   \n",
    "    ticker_df = get_historical_price(ticker, start_date, end_date, '1d', False)\n",
    "    \n",
    "    # Drop the data that has \n",
    "    ticker_df.dropna(inplace=True)\n",
    "    \n",
    "    print (f'Number of data extracted = {len(ticker_df)}')\n",
    "    \n",
    "    Short_TR_Data_Dict = {**Short_TR_Data_Dict,**{ticker:ticker_df}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Scraping historical data from  2000-01-02  to  2019-02-19\n",
      "Scraping ticker is SHV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haolu/anaconda3/envs/tensorflow/lib/python3.6/site-packages/ipykernel_launcher.py:37: FutureWarning: convert_objects is deprecated.  To re-infer data dtypes for object columns, use DataFrame.infer_objects()\n",
      "For all other conversions use the data-type specific converters pd.to_datetime, pd.to_timedelta and pd.to_numeric.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data extracted = 96\n",
      "\n",
      "\n",
      "Scraping historical data from  2000-01-02  to  2019-02-19\n",
      "Scraping ticker is VGSH\n",
      "Number of data extracted = 96\n",
      "\n",
      "\n",
      "Scraping historical data from  2000-01-02  to  2019-02-19\n",
      "Scraping ticker is SCHR\n",
      "Number of data extracted = 96\n"
     ]
    }
   ],
   "source": [
    "# Scrape Long Term Bonds\n",
    "Long_TR_Data_Dict = {}\n",
    "\n",
    "for ticker in Long_TR_Tickers:\n",
    "    \n",
    "    # Set up start and end date\n",
    "    start_date = datetime.date(2000, 1, 2)\n",
    "    end_date = datetime.date.today()\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"\")\n",
    "    print ('Scraping historical data from ',start_date,' to ',end_date)\n",
    "    print (f'Scraping ticker is {ticker}')\n",
    "   \n",
    "    ticker_df = get_historical_price(ticker, start_date, end_date, '1d',False)\n",
    "    \n",
    "    # Drop the data that has \n",
    "    ticker_df.dropna(inplace=True)\n",
    "    \n",
    "    print (f'Number of data extracted = {len(ticker_df)}')\n",
    "    \n",
    "    Long_TR_Data_Dict = {**Long_TR_Data_Dict,**{ticker:ticker_df}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
